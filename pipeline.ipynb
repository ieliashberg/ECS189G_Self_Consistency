{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b81f00",
   "metadata": {},
   "source": [
    "# Self-Consistency Pipeline\n",
    "## Reproducing Wang et al. 2022 — *Self-Consistency Improves Chain of Thought Reasoning in Language Models*\n",
    "\n",
    "**Greedy baseline:** `temperature=0, top_p=1, n=1`\n",
    "\n",
    "**Self-Consistency (majority vote over k sampled CoT paths):**\n",
    "\n",
    "| Model | Temperature | k values |\n",
    "|-------|-------------|----------|\n",
    "| GPT-3.5-Turbo | 0.7 | 1, 5, 10, 20, 40 |\n",
    "| UL2 (google/ul2) | 0.5 | 1, 5, 10, 20, 40 |\n",
    "| GPT-5.2 | 0.7 | 1, 5, 10, 20, 40 |\n",
    "\n",
    "**Benchmarks:** SVAMP, AQuA, GSM8K, StrategyQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8f9c6",
   "metadata": {},
   "source": [
    "## 0 — Imports\n",
    "\n",
    "Safely load modules from the existing codebase (some files have inline test code that\n",
    "references notebook-only variables, so we catch those errors during import)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7d6ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m _mod2.build_prompt = build_prompt\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43m_spec2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_mod2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WINTER26/ECS189G_Self_Consistency/aggregator.py:7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodel_fn\u001b[39m(prompts, model=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m):\n\u001b[32m     10\u001b[39m     responses = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WINTER26/ECS189G_Self_Consistency/.venv/lib/python3.11/site-packages/openai/_client.py:139\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    137\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    140\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import importlib.util, sys, os, re\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "_spec = importlib.util.spec_from_file_location(\"prompts\", \"prompts.py\")\n",
    "_mod = importlib.util.module_from_spec(_spec)\n",
    "try:\n",
    "    _spec.loader.exec_module(_mod)\n",
    "except NameError:\n",
    "    pass \n",
    "sys.modules[\"prompts\"] = _mod\n",
    "\n",
    "from prompts import BenchmarkType, build_prompt, FEW_SHOT_EXAMPLES\n",
    "\n",
    "_spec2 = importlib.util.spec_from_file_location(\"aggregator\", \"aggregator.py\")\n",
    "_mod2 = importlib.util.module_from_spec(_spec2)\n",
    "_mod2.BenchmarkType = BenchmarkType\n",
    "_mod2.build_prompt = build_prompt\n",
    "try:\n",
    "    _spec2.loader.exec_module(_mod2)\n",
    "except NameError:\n",
    "    pass  \n",
    "sys.modules[\"aggregator\"] = _mod2\n",
    "\n",
    "from aggregator import BenchmarkResults, extract_answer, grade_answer, aggregate\n",
    "\n",
    "from model_wrapper import ModelClient\n",
    "from plotting_wrapper import plot_self_consistency_curves\n",
    "\n",
    "print(\"All imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c0250",
   "metadata": {},
   "source": [
    "## 1 — Load & Parse Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8130ff19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load raw datasets (same splits used in benchmarks.ipynb)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m svamp_test       = \u001b[43mload_dataset\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtongyx361/svamp\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m aqua_test        = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mdeepmind/aqua_rat\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m gsm8k_test       = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mopenai/gsm8k\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load raw datasets (same splits used in benchmarks.ipynb)\n",
    "svamp_test       = load_dataset(\"tongyx361/svamp\", split=\"test\")\n",
    "aqua_test        = load_dataset(\"deepmind/aqua_rat\", split=\"test\")\n",
    "gsm8k_test       = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "strategy_qa_test = load_dataset(\"ChilleD/StrategyQA\", split=\"test\")\n",
    "\n",
    "# ── Parsing helpers (from benchmarks.ipynb) ────────────────────────────\n",
    "def parse_svamp_example(example):\n",
    "    example[\"question\"]     = example[\"Body\"] + \" \" + example[\"Question\"]\n",
    "    example[\"final_answer\"] = str(example[\"Answer\"])\n",
    "    return example\n",
    "\n",
    "def parse_aqua_example(example):\n",
    "    options = \"\\n\".join(example[\"options\"])\n",
    "    example[\"question\"]     = example[\"question\"] + \"\\nOptions:\\n\" + options\n",
    "    example[\"final_answer\"] = example[\"correct\"]\n",
    "    return example\n",
    "\n",
    "def parse_gsm8k_example(example):\n",
    "    split = example[\"answer\"].split(\"#### \")\n",
    "    example[\"final_answer\"] = split[-1].strip()\n",
    "    example[\"reasoning\"]    = split[0].strip()\n",
    "    return example\n",
    "\n",
    "def parse_strategy_qa_example(example):\n",
    "    example[\"final_answer\"] = str(example[\"answer\"])\n",
    "    return example\n",
    "\n",
    "# Apply parsers\n",
    "svamp_processed       = svamp_test.map(parse_svamp_example)\n",
    "aqua_processed        = aqua_test.map(parse_aqua_example)\n",
    "gsm8k_processed       = gsm8k_test.map(parse_gsm8k_example)\n",
    "strategy_qa_processed = strategy_qa_test.map(parse_strategy_qa_example)\n",
    "\n",
    "print(f\"SVAMP: {len(svamp_processed)},  AQuA: {len(aqua_processed)},  \"\n",
    "      f\"GSM8K: {len(gsm8k_processed)},  StrategyQA: {len(strategy_qa_processed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68832c3",
   "metadata": {},
   "source": [
    "## 2 — DataLoaders & Benchmark Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "svamp_dataloader       = DataLoader(svamp_processed,       batch_size=BATCH_SIZE)\n",
    "aqua_dataloader        = DataLoader(aqua_processed,        batch_size=BATCH_SIZE)\n",
    "gsm8k_dataloader       = DataLoader(gsm8k_processed,       batch_size=BATCH_SIZE)\n",
    "strategy_qa_dataloader = DataLoader(strategy_qa_processed, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Central mapping: name -> (dataloader, BenchmarkType)\n",
    "BENCHMARK_MAP = {\n",
    "    \"svamp\":       (svamp_dataloader,       BenchmarkType.SVAMP),\n",
    "    \"aqua\":        (aqua_dataloader,        BenchmarkType.AQUA),\n",
    "    \"gsm8k\":       (gsm8k_dataloader,       BenchmarkType.GSM8K),\n",
    "    \"strategy_qa\": (strategy_qa_dataloader, BenchmarkType.STRATEGY_QA),\n",
    "}\n",
    "\n",
    "# Quick sanity check\n",
    "for name, (dl, btype) in BENCHMARK_MAP.items():\n",
    "    batch = next(iter(dl))\n",
    "    print(f\"{name:>12s}  Q: {batch['question'][0][:80]}…  A: {batch['final_answer'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e696310",
   "metadata": {},
   "source": [
    "## 3 — Majority-Vote & Self-Consistency Aggregation\n",
    "\n",
    "The original `aggregate()` in `aggregator.py` does greedy (1-sample) evaluation.\n",
    "Below we add **`majority_vote`** and **`aggregate_self_consistency`** which sample\n",
    "*k* CoT paths per question and take a majority vote — the core of Wang et al. 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(answers):\n",
    "    \"\"\"Return the most common non-None answer (ties broken arbitrarily).\"\"\"\n",
    "    valid = [a for a in answers if a is not None]\n",
    "    if not valid:\n",
    "        return None\n",
    "    return Counter(valid).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def aggregate_self_consistency(\n",
    "    client,\n",
    "    model_name,\n",
    "    dataloader,\n",
    "    benchmark,\n",
    "    temperature,\n",
    "    k,\n",
    "    cot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Self-consistency evaluation: for each question, sample *k* CoT\n",
    "    reasoning paths, extract an answer from each, and return the\n",
    "    majority-vote answer.\n",
    "    \"\"\"\n",
    "    results = BenchmarkResults(name=benchmark.value)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        questions    = batch[\"question\"]\n",
    "        gold_answers = batch[\"final_answer\"]\n",
    "\n",
    "        for q, gold in zip(questions, gold_answers):\n",
    "            prompt = build_prompt(q, benchmark, cot=cot)\n",
    "            responses = client.generate(\n",
    "                model_name=model_name,\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                num_samples=k,\n",
    "            )\n",
    "\n",
    "            extracted = [extract_answer(r, benchmark) for r in responses]\n",
    "            predicted = majority_vote(extracted)\n",
    "            correct   = grade_answer(predicted, gold, benchmark)\n",
    "            results.total += 1\n",
    "            if correct:\n",
    "                results.correct += 1\n",
    "            else:\n",
    "                results.failures.append({\n",
    "                    \"question\": q,\n",
    "                    \"predicted\": predicted,\n",
    "                    \"gold\": gold,\n",
    "                    \"raw_responses\": responses,\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"majority_vote and aggregate_self_consistency defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060819c",
   "metadata": {},
   "source": [
    "## 4 — Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ModelClient()\n",
    "\n",
    "# ── Self-consistency k values from the paper ──────────────────────────\n",
    "K_VALUES = [1, 5, 10, 20, 40]\n",
    "\n",
    "# ── Per-model sampling temperatures ───────────────────────────────────\n",
    "MODEL_CONFIGS = {\n",
    "    \"gpt-3.5-turbo\": 0.7,\n",
    "    \"google/ul2\":    0.5,\n",
    "    \"gpt-5.2\":       0.7,\n",
    "}\n",
    "\n",
    "RESULTS_CSV = \"results.csv\"\n",
    "PLOTS_DIR   = \"plots\"\n",
    "\n",
    "print(f\"Models:     {list(MODEL_CONFIGS.keys())}\")\n",
    "print(f\"Datasets:   {list(BENCHMARK_MAP.keys())}\")\n",
    "print(f\"k values:   {K_VALUES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f2543",
   "metadata": {},
   "source": [
    "## 5 — Run Greedy Baselines (`temperature=0, n=1`)\n",
    "\n",
    "Uses the existing `aggregate()` from `aggregator.py` with a thin wrapper\n",
    "around `ModelClient.generate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1477f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for model_name in MODEL_CONFIGS:\n",
    "    # Wrap ModelClient.generate to match aggregate()'s expected interface:\n",
    "    #   model_fn(prompts: List[str]) -> List[str]   (one response per prompt)\n",
    "    def _greedy_fn(prompts, _model=model_name):\n",
    "        return [\n",
    "            client.generate(\n",
    "                model_name=_model,\n",
    "                prompt=p,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                num_samples=1,\n",
    "            )[0]\n",
    "            for p in prompts\n",
    "        ]\n",
    "\n",
    "    for ds_name, (dataloader, btype) in BENCHMARK_MAP.items():\n",
    "        print(f\"[greedy] {model_name} / {ds_name} … \", end=\"\", flush=True)\n",
    "        result = aggregate(_greedy_fn, dataloader, btype, cot=True)\n",
    "        print(result)\n",
    "        rows.append({\n",
    "            \"model\":    model_name,\n",
    "            \"dataset\":  ds_name,\n",
    "            \"method\":   \"greedy\",\n",
    "            \"k\":        1,\n",
    "            \"accuracy\": result.accuracy,\n",
    "        })\n",
    "\n",
    "greedy_df = pd.DataFrame(rows)\n",
    "greedy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a38b7",
   "metadata": {},
   "source": [
    "## 6 — Run Self-Consistency (`k ∈ {1, 5, 10, 20, 40}`)\n",
    "\n",
    "For each model × dataset × k, sample *k* CoT paths and take a majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38565c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_rows = []\n",
    "\n",
    "for model_name, temperature in MODEL_CONFIGS.items():\n",
    "    for ds_name, (dataloader, btype) in BENCHMARK_MAP.items():\n",
    "        for k in K_VALUES:\n",
    "            print(f\"[self-consistency] {model_name} / {ds_name} / k={k} … \",\n",
    "                  end=\"\", flush=True)\n",
    "            result = aggregate_self_consistency(\n",
    "                client=client,\n",
    "                model_name=model_name,\n",
    "                dataloader=dataloader,\n",
    "                benchmark=btype,\n",
    "                temperature=temperature,\n",
    "                k=k,\n",
    "                cot=True,\n",
    "            )\n",
    "            print(result)\n",
    "            sc_rows.append({\n",
    "                \"model\":    model_name,\n",
    "                \"dataset\":  ds_name,\n",
    "                \"method\":   \"self_consistency\",\n",
    "                \"k\":        k,\n",
    "                \"accuracy\": result.accuracy,\n",
    "            })\n",
    "\n",
    "sc_df = pd.DataFrame(sc_rows)\n",
    "sc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc184b",
   "metadata": {},
   "source": [
    "## 7 — Save Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff8b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([greedy_df, sc_df], ignore_index=True)\n",
    "all_df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Saved {len(all_df)} rows to {RESULTS_CSV}\")\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4147a",
   "metadata": {},
   "source": [
    "## 8 — Plot Results\n",
    "\n",
    "Uses `plot_self_consistency_curves` from `plotting_wrapper.py` to generate\n",
    "one accuracy-vs-k chart per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_self_consistency_curves(\n",
    "    df=all_df,\n",
    "    output_dir=PLOTS_DIR,\n",
    "    k_values=K_VALUES,\n",
    ")\n",
    "\n",
    "# Also display inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "\n",
    "for img_path in sorted(Path(PLOTS_DIR).glob(\"*.png\")):\n",
    "    img = mpimg.imread(str(img_path))\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(img_path.stem)\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c39a9",
   "metadata": {},
   "source": [
    "## 9 — Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0671cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = all_df.pivot_table(\n",
    "    index=[\"model\", \"method\"],\n",
    "    columns=[\"dataset\", \"k\"],\n",
    "    values=\"accuracy\",\n",
    ")\n",
    "pivot.style.format(\"{:.1%}\").set_caption(\"Accuracy by model / method / dataset / k\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECS189G_Self_Consistency (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
